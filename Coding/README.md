# Machine Learning

## Difference between Gradient Boosting DT and XGBoost

| **优化点**     | **XGBoost**           | **普通 GBM**           | **优势**           |
| -------------- | --------------------- | ---------------------- | ------------------ |
| **梯度计算**   | 二阶梯度（Hessian）   | 一阶梯度               | **更快收敛**       |
| **正则化**     | L1 + L2               | 无                     | **防止过拟合**     |
| **并行计算**   | 多线程 & GPU          | 串行                   | **训练速度提升**   |
| **缺失值处理** | 自动选择最优路径      | 需手动填充             | **提高数据鲁棒性** |
| **剪枝策略**   | 预剪枝（Pre-pruning） | 后剪枝（Post-pruning） | **减少无效分裂**   |

**计算优化**：XGBoost 采用 **二阶导数优化（Taylor Expansion）**，比普通 GBM 只用 **一阶梯度** 计算更快收敛。

**正则化**：XGBoost 添加了 **L1 & L2 正则项**，避免过拟合。

**并行计算**：XGBoost **支持多线程 & GPU 加速**，大大提升训练速度。

**缺失值处理**：XGBoost **自动处理缺失值**，无需手动填充。

**剪枝（Pruning）**：XGBoost 采用 **预剪枝（Pre-pruning）**，而普通 GBM 采用 **后剪枝（Post-pruning）**，XGBoost 计算更高效。

## Difference And Similarity between Ridge and Lasso

Lasso（Least Absolute Shrinkage and Selection Operator）和 Ridge（岭回归）都是**线性回归的正则化方法**，主要用于防止模型过拟合并提高泛化能力。

**1. 相同点**

1. **都是正则化方法**：

   - 通过向损失函数中加入**惩罚项**来约束模型的参数，避免过拟合。

2. **都用于处理多重共线性问题**：

   - 当特征之间高度相关时，普通最小二乘回归（OLS）可能会导致不稳定的系数，Lasso 和 Ridge 能有效缓解这个问题。

3. **超参数控制正则化强度**：

   - 两者都引入一个超参数 

     α\alphaα

      来控制正则化的强度：

     - α\alphaα 较大 → 更强的正则化，模型更简单，偏差增大，方差减小。
     - α\alphaα 较小 → 更弱的正则化，模型更复杂，偏差减小，方差增大。

4. **损失函数结构类似**：

   - 都是**线性回归的损失函数 + 正则化项**。

**2. 不同点**

|                | **Lasso (L1 正则化)**                                    | **Ridge (L2 正则化)**                                    |
| -------------- | -------------------------------------------------------- | -------------------------------------------------------- |
| **正则化项**   | $\lambda \sum |w_i|$                                     | $\lambda \sum {w_i}^2$                                   |
| **作用**       | 产生**稀疏解**，可以用于**特征选择**                     | 不会让特征权重变成 0，适用于高维度但不需要特征选择的场景 |
| **系数收缩**   | 一些特征的权重会被强制降为 0（稀疏）                     | 所有权重都被缩小，但不会变成 0                           |
| **适用场景**   | 适用于特征冗余较多的情况（可以自动筛选特征）             | 适用于多重共线性但仍希望保留所有特征                     |
| **计算复杂度** | 由于 L1 范数的不可导性，需要用 **坐标下降法** 等优化算法 | L2 正则化是二次可导的，可直接用**梯度下降**              |

# Deep Learning

## dropout层有什么用？其在训练和测试阶段有什么区别

Dropout 是一种**正则化**技术，旨在减少神经网络的过拟合（overfitting）。其核心思想是在**训练过程中**，以一定的概率 ppp 随机丢弃（置零）一部分神经元，使模型不会过度依赖某些特定的特征，而是学会更具鲁棒性的特征表示。

1. **训练阶段（Training Phase）**：
   - 对于每个神经元，以概率 ppp（一般设为 0.2~0.5）随机“丢弃”它，即将其输出置为 0。
   - 这样可以防止神经元之间的共适应（co-adaptation），迫使网络学习更通用的特征。
   - 剩下的激活神经元的输出会**按 1/(1-p) 进行缩放**，以保持输出的期望值不变。
2. **测试阶段（Inference Phase）**：
   - Dropout 层不会再丢弃任何神经元，而是让所有神经元都参与计算。
   - 由于训练时神经元的输出被缩放了 1/(1−p)1/(1-p)1/(1−p)，所以在测试阶段不需要额外缩放。
