## Difference between Gradient Boosting DT and XGBoost

| **优化点**     | **XGBoost**           | **普通 GBM**           | **优势**           |
| -------------- | --------------------- | ---------------------- | ------------------ |
| **梯度计算**   | 二阶梯度（Hessian）   | 一阶梯度               | **更快收敛**       |
| **正则化**     | L1 + L2               | 无                     | **防止过拟合**     |
| **并行计算**   | 多线程 & GPU          | 串行                   | **训练速度提升**   |
| **缺失值处理** | 自动选择最优路径      | 需手动填充             | **提高数据鲁棒性** |
| **剪枝策略**   | 预剪枝（Pre-pruning） | 后剪枝（Post-pruning） | **减少无效分裂**   |

**计算优化**：XGBoost 采用 **二阶导数优化（Taylor Expansion）**，比普通 GBM 只用 **一阶梯度** 计算更快收敛。

**正则化**：XGBoost 添加了 **L1 & L2 正则项**，避免过拟合。

**并行计算**：XGBoost **支持多线程 & GPU 加速**，大大提升训练速度。

**缺失值处理**：XGBoost **自动处理缺失值**，无需手动填充。

**剪枝（Pruning）**：XGBoost 采用 **预剪枝（Pre-pruning）**，而普通 GBM 采用 **后剪枝（Post-pruning）**，XGBoost 计算更高效。

